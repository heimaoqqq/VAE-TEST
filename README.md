## Training an unconditional latent diffusion model

Creating a training image set is [described in a different document](https://huggingface.co/docs/datasets/image_process#image-datasets).

### Cloning to local
```bash
git clone https://github.com/zyinghua/uncond-image-generation-ldm.git
```

Then call:
```bash
cd uncond-image-generation-ldm
```

### Installing the dependencies

Before running the scripts, make sure to install the library's training dependencies:
```bash
pip install -r requirements.txt
```

And initialize an [ğŸ¤—Accelerate](https://github.com/huggingface/accelerate/) environment with:

```bash
accelerate config
```

### Change Pretrained VAE settings
You can specify which pretrained VAE model to use by changing the `VAE_PRETRAINED_PATH` and `VAE_KWARGS` variables in `train.py`, at the top.

### Unconditional Flowers

An examplar command to train a DDPM UNet model on the Oxford Flowers dataset, without using GPUs:

```bash
accelerate launch train.py \
  --dataset_name="huggan/flowers-102-categories" \
  --resolution=256 \
  --output_dir="ddpm-ema-flowers-256" \
  --train_batch_size=16 \
  --num_epochs=150 \
  --gradient_accumulation_steps=1 \
  --use_ema \
  --learning_rate=1e-4 \
  --lr_warmup_steps=500 \
  --mixed_precision=no \
```

### Training with multiple GPUs

`accelerate` allows for seamless multi-GPU training. After setting up with `accelerate config`,
simply add `--multi_gpu` in the command. For more information, follow the instructions [here](https://huggingface.co/docs/accelerate/basic_tutorials/launch)
for running distributed training with `accelerate`. Here is an example command:

```bash
accelerate launch --multi_gpu train.py \
  --dataset_name="huggan/flowers-102-categories" \
  --resolution=256 \
  --output_dir="ddpm-ema-flowers-256" \
  --train_batch_size=16 \
  --num_epochs=150 \
  --gradient_accumulation_steps=1 \
  --use_ema \
  --learning_rate=1e-4 \
  --lr_warmup_steps=500 \
  --mixed_precision=no \
```

To be able to use Weights and Biases (`wandb`) as a logger you need to install the library: `pip install wandb`.

### Using your own data

To use your own dataset, there are 3 ways:
- you can either provide your own folder as `--train_data_dir`
- or you can provide your own .zip file containing the data as `--train_data_files`
- or you can upload your dataset to the hub (possibly as a private repo, if you prefer so), and simply pass the `--dataset_name` argument.

Below, we explain both in more detail.

#### Provide the dataset as a folder/zip file

If you provide your own folders with images, the script expects the following directory structure:

```bash
data_dir/xxx.png
data_dir/xxy.png
data_dir/[...]/xxz.png
```

In other words, the script will take care of gathering all images inside the folder. You can then run the script like this:

```bash
accelerate launch train.py \
    --train_data_dir <path-to-train-directory> \
    <other-arguments>
```

Or (if it is a zip file):
```bash
accelerate launch train.py \
    --train_data_files <path-to-train-zip-file> \
    <other-arguments>
```

Internally, the script will use the [`ImageFolder`](https://huggingface.co/docs/datasets/v2.0.0/en/image_process#imagefolder) feature which will automatically turn the folders into ğŸ¤— Dataset objects.

Official [diffusers](https://github.com/huggingface/diffusers) repo also has a pipeline for uncond ldm that can be found [here](https://github.com/huggingface/diffusers/tree/main/src/diffusers/pipelines/deprecated/latent_diffusion_uncond).

# å¾®å¤šæ™®å‹’æ—¶é¢‘å›¾æ•°æ®å¢å¹¿ (Micro-Doppler Spectrogram Data Augmentation)

æœ¬é¡¹ç›®åŸºäºæ½œåœ¨æ‰©æ•£æ¨¡å‹(Latent Diffusion Model)å®ç°å¾®å¤šæ™®å‹’æ—¶é¢‘å›¾åƒçš„æ•°æ®å¢å¹¿ï¼Œè§£å†³æ­¥æ€å¾®å¤šæ™®å‹’æ—¶é¢‘å›¾æ•°æ®é‡ä¸è¶³çš„é—®é¢˜ã€‚

## é¡¹ç›®æ¦‚è¿°

å¾®å¤šæ™®å‹’æ—¶é¢‘å›¾æ˜¯é›·è¾¾ä¿¡å·å¤„ç†ä¸­çš„é‡è¦æ•°æ®å½¢å¼ï¼Œç‰¹åˆ«æ˜¯åœ¨æ­¥æ€è¯†åˆ«ç­‰åº”ç”¨ä¸­ã€‚ç„¶è€Œï¼Œç”±äºé‡‡é›†æˆæœ¬é«˜ã€æ ·æœ¬æ•°é‡æœ‰é™ç­‰åŸå› ï¼Œæ•°æ®é‡ä¸è¶³æˆä¸ºåˆ¶çº¦ç›¸å…³ç ”ç©¶çš„ç“¶é¢ˆã€‚æœ¬é¡¹ç›®ä½¿ç”¨ä¸¤é˜¶æ®µç”Ÿæˆæ¨¡å‹è¿›è¡Œæ•°æ®å¢å¹¿ï¼š
1. ç¬¬ä¸€é˜¶æ®µï¼šVQ-VAE (Vector Quantized Variational Autoencoder) å°†é«˜ç»´å›¾åƒå‹ç¼©åˆ°æ½œåœ¨ç©ºé—´
2. ç¬¬äºŒé˜¶æ®µï¼šåœ¨æ½œåœ¨ç©ºé—´ä¸­ä½¿ç”¨æ¡ä»¶æ‰©æ•£æ¨¡å‹ç”Ÿæˆæ–°æ ·æœ¬

## æ¨¡å‹æ¶æ„

ä¸‹å›¾å±•ç¤ºäº†å¾®å¤šæ™®å‹’æ—¶é¢‘å›¾æ•°æ®å¢å¹¿çš„æ•´ä½“æ¶æ„ï¼š

```mermaid
flowchart TD
    subgraph "ç¬¬ä¸€é˜¶æ®µ: VQ-VAE"
        A[å¾®å¤šæ™®å‹’æ—¶é¢‘å›¾] --> B[ç¼–ç å™¨]
        B --> C[é‡åŒ–å±‚]
        C --> D[æ½œåœ¨è¡¨ç¤º]
        D --> E[è§£ç å™¨]
        E --> F[é‡å»ºå›¾åƒ]
    end
    
    subgraph "ç¬¬äºŒé˜¶æ®µ: æ‰©æ•£æ¨¡å‹"
        D --> G[æ·»åŠ å™ªå£°]
        G --> H[UNeté¢„æµ‹å™ªå£°]
        H --> I[å»å™ª]
        I --> J[æ½œåœ¨ç©ºé—´é‡‡æ ·]
    end
    
    J --> K[VQ-VAEè§£ç ]
    K --> L[ç”Ÿæˆçš„å¾®å¤šæ™®å‹’æ—¶é¢‘å›¾]
    
    style A fill:#f9d5e5,stroke:#333,stroke-width:2px
    style F fill:#eeeeee,stroke:#333,stroke-width:1px
    style L fill:#d5f9e5,stroke:#333,stroke-width:2px
```

è¯¥æ¶æ„é¦–å…ˆä½¿ç”¨VQ-VAEå°†å¾®å¤šæ™®å‹’æ—¶é¢‘å›¾å‹ç¼©åˆ°æ½œåœ¨ç©ºé—´ï¼Œç„¶ååœ¨æ½œåœ¨ç©ºé—´ä¸­åº”ç”¨æ‰©æ•£æ¨¡å‹è¿›è¡Œé‡‡æ ·ï¼Œæœ€åé€šè¿‡VQ-VAEçš„è§£ç å™¨ç”Ÿæˆæ–°çš„å¾®å¤šæ™®å‹’æ—¶é¢‘å›¾æ ·æœ¬ã€‚

## å¤„ç†æµç¨‹

æ•´ä¸ªæ•°æ®å¢å¹¿çš„å¤„ç†æµç¨‹å¦‚ä¸‹ï¼š

```mermaid
graph TD
    A[åŸå§‹å¾®å¤šæ™®å‹’æ—¶é¢‘å›¾æ•°æ®] --> B[æ•°æ®é¢„å¤„ç†]
    B --> C[é¢„å¤„ç†åçš„æ•°æ®é›†]
    C --> D[VQ-VAEç¼–ç ]
    D --> E[æ½œåœ¨ç©ºé—´è¡¨ç¤º]
    E --> F[æ‰©æ•£æ¨¡å‹è®­ç»ƒ]
    F --> G[è®­ç»ƒå¥½çš„æ¨¡å‹]
    G --> H[æ‰©æ•£æ¨¡å‹é‡‡æ ·]
    H --> I[æ½œåœ¨ç©ºé—´æ ·æœ¬]
    I --> J[VQ-VAEè§£ç ]
    J --> K[ç”Ÿæˆçš„å¾®å¤šæ™®å‹’æ—¶é¢‘å›¾]
    
    style A fill:#ffcccb,stroke:#333,stroke-width:1px
    style C fill:#c2f0c2,stroke:#333,stroke-width:1px
    style E fill:#c2d6f0,stroke:#333,stroke-width:1px
    style G fill:#f0e6c2,stroke:#333,stroke-width:1px
    style K fill:#d8c2f0,stroke:#333,stroke-width:1px
```

## ç‰¹ç‚¹

- æ”¯æŒ256x256å½©è‰²å¾®å¤šæ™®å‹’æ—¶é¢‘å›¾åƒç”Ÿæˆ
- ä¸¤é˜¶æ®µç”Ÿæˆæ¶æ„ï¼Œä¿è¯ç”Ÿæˆè´¨é‡å’Œå¤šæ ·æ€§
- æ”¯æŒæ‰¹é‡ç”Ÿæˆå’Œè‡ªå®šä¹‰æ¨ç†å‚æ•°
- é€‚ç”¨äºæ­¥æ€å¾®å¤šæ™®å‹’æ—¶é¢‘å›¾æ•°æ®å¢å¹¿

## å®‰è£…ä¾èµ–

```bash
pip install -r requirements.txt
```

## æ•°æ®å‡†å¤‡

1. å°†æ‚¨çš„å¾®å¤šæ™®å‹’æ—¶é¢‘å›¾æ•°æ®é›†ç»„ç»‡ä¸ºä»¥ä¸‹ç»“æ„ï¼š

```
data_dir/
  â”œâ”€â”€ user_001/
  â”‚   â”œâ”€â”€ image_001.png
  â”‚   â”œâ”€â”€ image_002.png
  â”‚   â””â”€â”€ ...
  â”œâ”€â”€ user_002/
  â”‚   â”œâ”€â”€ image_001.png
  â”‚   â””â”€â”€ ...
  â””â”€â”€ ...
```

2. è¿è¡Œæ•°æ®é¢„å¤„ç†è„šæœ¬ï¼ˆç¡®ä¿å›¾åƒå°ºå¯¸ä¸º256x256ï¼‰ï¼š

```bash
accelerate launch train.py \
  --train_data_dir path/to/your/data_dir \
  --resolution 256 \
  --output_dir microdoppler_ldm_model \
  --train_batch_size 16 \
  --num_epochs 150 \
  --use_ema \
  --learning_rate 1e-4 \
  --lr_warmup_steps 500
```

## æ•°æ®é¢„å¤„ç†

å¾®å¤šæ™®å‹’æ—¶é¢‘å›¾æ•°æ®é¢„å¤„ç†æ˜¯æ•´ä¸ªæµç¨‹ä¸­çš„é‡è¦ä¸€æ­¥ï¼Œç¡®ä¿è¾“å…¥æ•°æ®çš„ä¸€è‡´æ€§å’Œè´¨é‡ã€‚é¢„å¤„ç†æ­¥éª¤åŒ…æ‹¬ï¼š

1. **æ•°æ®æ ¼å¼ç»Ÿä¸€**ï¼šå°†æ‰€æœ‰å›¾åƒè½¬æ¢ä¸ºRGBæ ¼å¼ï¼Œç¡®ä¿é€šé“æ•°ä¸€è‡´
2. **å°ºå¯¸è°ƒæ•´**ï¼šå°†æ‰€æœ‰å›¾åƒè°ƒæ•´ä¸º256x256åƒç´ ï¼Œä¾¿äºæ¨¡å‹å¤„ç†
3. **æ•°æ®å½’ä¸€åŒ–**ï¼šå°†åƒç´ å€¼å½’ä¸€åŒ–åˆ°[-1, 1]èŒƒå›´ï¼Œæé«˜è®­ç»ƒç¨³å®šæ€§
4. **ç›®å½•ç»“æ„æ•´ç†**ï¼šä¿æŒç”¨æˆ·ç›®å½•ç»“æ„ï¼Œä¾¿äºåç»­åˆ†æ

### é¢„å¤„ç†å‘½ä»¤

å¯ä»¥ä½¿ç”¨ä»¥ä¸‹å‘½ä»¤å•ç‹¬æ‰§è¡Œæ•°æ®é¢„å¤„ç†ï¼š

```bash
python train.py \
  --preprocess_only \
  --raw_data_dir path/to/raw/data \
  --processed_data_dir path/to/processed/data \
  --resolution 256 \
  --max_samples 1000  # å¯é€‰ï¼Œé™åˆ¶å¤„ç†æ ·æœ¬æ•°é‡
```

é¢„å¤„ç†åçš„æ•°æ®å°†ä¿æŒåŸå§‹ç›®å½•ç»“æ„ï¼Œä½†å›¾åƒä¼šè¢«æ ‡å‡†åŒ–ä¸º256x256çš„RGBæ ¼å¼ã€‚

### é¢„å¤„ç†ä¸è®­ç»ƒä¸€ä½“åŒ–

ä¹Ÿå¯ä»¥åœ¨è®­ç»ƒæ—¶è‡ªåŠ¨æ‰§è¡Œé¢„å¤„ç†ï¼š

```bash
python train.py \
  --raw_data_dir path/to/raw/data \
  --processed_data_dir path/to/processed/data \
  --resolution 256 \
  --output_dir microdoppler_ldm_model \
  --train_batch_size 16 \
  --num_epochs 150 \
  --use_ema \
  --learning_rate 1e-4 \
  --lr_warmup_steps 500
```

è¿™å°†é¦–å…ˆé¢„å¤„ç†æ•°æ®ï¼Œç„¶åä½¿ç”¨å¤„ç†åçš„æ•°æ®è¿›è¡Œæ¨¡å‹è®­ç»ƒã€‚

## è®­ç»ƒæ¨¡å‹

1. è®­ç»ƒVQ-VAEæ¨¡å‹ï¼ˆå¦‚æœæ‚¨æƒ³ä½¿ç”¨è‡ªå®šä¹‰çš„VAEæ¨¡å‹ï¼‰ï¼š
   - ä¿®æ”¹`train.py`ä¸­çš„`VAE_PRETRAINED_PATH`å’Œ`VAE_KWARGS`å˜é‡
   - æˆ–è€…ä½¿ç”¨é»˜è®¤çš„é¢„è®­ç»ƒVQ-VAEæ¨¡å‹

2. è®­ç»ƒæ‰©æ•£æ¨¡å‹ï¼š

```bash
accelerate launch train.py \
  --train_data_dir path/to/your/data_dir \
  --resolution 256 \
  --output_dir microdoppler_ldm_model \
  --train_batch_size 16 \
  --num_epochs 150 \
  --use_ema \
  --learning_rate 1e-4 \
  --lr_warmup_steps 500
```

## ç”Ÿæˆå¾®å¤šæ™®å‹’æ—¶é¢‘å›¾åƒ

1. ä¿®æ”¹`inference.py`ä¸­çš„`model_id`ä¸ºæ‚¨çš„æ¨¡å‹è·¯å¾„
2. è¿è¡Œæ¨ç†è„šæœ¬ï¼š

```bash
python inference.py
```

3. ç”Ÿæˆçš„å›¾åƒå°†ä¿å­˜åœ¨`generated_images`ç›®å½•ä¸­

## è‡ªå®šä¹‰ç”Ÿæˆå‚æ•°

æ‚¨å¯ä»¥é€šè¿‡ä¿®æ”¹`inference.py`ä¸­çš„ä»¥ä¸‹å‚æ•°æ¥è‡ªå®šä¹‰ç”Ÿæˆè¿‡ç¨‹ï¼š

```python
batch_size = 16  # ä¸€æ¬¡ç”Ÿæˆçš„å›¾åƒæ•°é‡
num_inference_steps = 1000  # æ¨ç†æ­¥æ•°
output_dir = "generated_images"  # è¾“å‡ºç›®å½•
```

## å¼•ç”¨

å¦‚æœæ‚¨åœ¨ç ”ç©¶ä¸­ä½¿ç”¨äº†æœ¬é¡¹ç›®ï¼Œè¯·å¼•ç”¨ä»¥ä¸‹è®ºæ–‡ï¼š

```
@article{rombach2022highresolution,
  title={High-Resolution Image Synthesis with Latent Diffusion Models},
  author={Rombach, Robin and Blattmann, Andreas and Lorenz, Dominik and Esser, Patrick and Ommer, BjÃ¶rn},
  journal={arXiv preprint arXiv:2112.10752},
  year={2022}
}
```

## è®¸å¯è¯

æœ¬é¡¹ç›®åŸºäºMITè®¸å¯è¯å¼€æºã€‚

